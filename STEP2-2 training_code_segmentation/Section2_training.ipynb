{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai; import nibabel; import tqdm\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21번과 비교실험 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    Resized,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandZoomd,\n",
    "    RandFlip,\n",
    "    RandRotate90,\n",
    "    RandAdjustContrast,\n",
    "    RandShiftIntensity,\n",
    "    RandSimulateLowResolutiond\n",
    "\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import (SwinUNETR, UNETR, UNet, DynUNet, SegResNet)\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "def get_kernels_strides(patch_size, spacing):\n",
    "    \"\"\"\n",
    "    This function is only used for decathlon datasets with the provided patch sizes.\n",
    "    When refering this method for other tasks, please ensure that the patch size for each spatial dimension should\n",
    "    be divisible by the product of all strides in the corresponding dimension.\n",
    "    In addition, the minimal spatial size should have at least one dimension that has twice the size of\n",
    "    the product of all strides. For patch sizes that cannot find suitable strides, an error will be raised.\n",
    "\n",
    "    \"\"\"\n",
    "    sizes, spacings = patch_size, spacing\n",
    "    input_size = sizes\n",
    "    strides, kernels = [], []\n",
    "    while True:\n",
    "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "        stride = [2 if ratio <= 2 and size >= 8 else 1 for (ratio, size) in zip(spacing_ratio, sizes)]\n",
    "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "        if all(s == 1 for s in stride):\n",
    "            break\n",
    "        for idx, (i, j) in enumerate(zip(sizes, stride)):\n",
    "            if i % j != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Patch size is not supported, please try to modify the size {input_size[idx]} in the spatial dimension {idx}.\"\n",
    "                )\n",
    "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "        kernels.append(kernel)\n",
    "        strides.append(stride)\n",
    "\n",
    "    strides.insert(0, len(spacings) * [1])\n",
    "    kernels.append(len(spacings) * [3])\n",
    "    return kernels, strides\n",
    "class PolyLRScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr: float, max_steps: int, exponent: float = 0.9, current_step: int = None):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.max_steps = max_steps\n",
    "        self.exponent = exponent\n",
    "        self.ctr = 0\n",
    "        super().__init__(optimizer, current_step if current_step is not None else -1, False)\n",
    "\n",
    "    def step(self, current_step=None):\n",
    "        if current_step is None or current_step == -1:\n",
    "            current_step = self.ctr\n",
    "            self.ctr += 1\n",
    "\n",
    "        new_lr = self.initial_lr * (1 - current_step / self.max_steps) ** self.exponent\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def oar_run_train(json_name):\n",
    "    \n",
    "    root_dir = r\"/media/cwlg102/58d6b55d-621b-4e86-800e-e2e14dd3bf07/project/aortaseg/data/new_final/zone2/\"\n",
    "    data_root_dir = r\"/media/cwlg102/58d6b55d-621b-4e86-800e-e2e14dd3bf07/project/aortaseg/data/new_final/zone2/\"\n",
    "    data_dir = root_dir + r\"/\"\n",
    "    num_samples = 2\n",
    "    model_savepath = os.path.join(data_root_dir, \"20240923model_13_%s\" %json_name)\n",
    "    loss_savepath = os.path.join(data_root_dir, \"20240923loss_13_%s\" %json_name)\n",
    "    os.makedirs(model_savepath)\n",
    "    os.makedirs(loss_savepath)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    space_x, space_y, space_z = 1.1, 1.1, 2.0\n",
    "    a_min, a_max, b_min, b_max = -175, 350, 0, 1\n",
    "    spatial_size_xyz = (128, 128, 112)\n",
    "    \n",
    "    train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "                # Orientationd(keys=[\"image\", \"label\"], axcodes=\"LPS\"),\n",
    "                EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=False),\n",
    "                RandRotate90d(\n",
    "                     keys=[\"image\", \"label\"],\n",
    "                     prob=0.2,\n",
    "                     max_k=3,\n",
    "                ),\n",
    "                RandZoomd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    min_zoom=0.7,\n",
    "                    max_zoom=1.4,\n",
    "                    mode=(\"trilinear\", \"nearest\"),\n",
    "                    align_corners=(True, None),\n",
    "                    prob=0.20,\n",
    "                ),\n",
    "                RandFlipd([\"image\", \"label\"], spatial_axis=[0], prob=0.5),\n",
    "                RandFlipd([\"image\", \"label\"], spatial_axis=[1], prob=0.5),\n",
    "                RandFlipd([\"image\", \"label\"], spatial_axis=[2], prob=0.5),\n",
    "                # RandCropByPosNegLabeld(\n",
    "                #     keys=[\"image\", \"label\"],\n",
    "                #     label_key=\"label\",\n",
    "                #     spatial_size=spatial_size_xyz,\n",
    "                #     pos=1,\n",
    "                #     neg=1,\n",
    "                #     num_samples=num_samples,\n",
    "                #     image_key=\"image\",\n",
    "                #     image_threshold=0,\n",
    "                # ),\n",
    "                RandSpatialCropSamplesd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    roi_size=spatial_size_xyz,\n",
    "                    num_samples=num_samples,\n",
    "                    random_size=False\n",
    "                ),\n",
    "                RandSimulateLowResolutiond(keys=[\"image\"], prob=0.25),\n",
    "                RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.1),\n",
    "                RandGaussianSmoothd(\n",
    "                    keys=[\"image\"],\n",
    "                    sigma_x=(0.5, 1.15),\n",
    "                    sigma_y=(0.5, 1.15),\n",
    "                    sigma_z=(0.5, 1.15),\n",
    "                    prob=0.2    ,\n",
    "                )\n",
    "                \n",
    "                \n",
    "                # RandShiftIntensityd(\n",
    "                #     keys=[\"image\"],\n",
    "                #     offsets=0.10,\n",
    "                #     prob=0.10,\n",
    "                # ),\n",
    "            ]\n",
    "            ) \n",
    "    \n",
    "        \n",
    "    \n",
    "    val_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "            # ScaleIntensityRanged(keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "            # Orientationd(keys=[\"image\", \"label\"], axcodes=\"LPS\"),\n",
    "            EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "            ScaleIntensityRanged(keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"LPS\"),\n",
    "            Spacingd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                pixdim=(space_x, space_y, space_z),\n",
    "                mode=(\"bilinear\", \"nearest\"),\n",
    "            ),\n",
    "            EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    split_json = json_name\n",
    "    \n",
    "    datasets = data_dir + split_json\n",
    "    datalist = load_decathlon_datalist(datasets, True, \"training\")\n",
    "    val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "    train_ds = CacheDataset(\n",
    "        data=datalist,\n",
    "        transform=train_transforms,\n",
    "        cache_num=40,\n",
    "        cache_rate=1.0,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    train_loader = ThreadDataLoader(train_ds, num_workers=0, batch_size=1, shuffle=True)\n",
    "    val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_num=0, cache_rate=1.0, num_workers=4)\n",
    "    val_loader = ThreadDataLoader(val_ds, num_workers=0, batch_size=1)\n",
    "\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    OAR_nums_plus_one = 9\n",
    "    patch_size = list(spatial_size_xyz)\n",
    "    spacing = [1.0, 1.0, 1.0]\n",
    "    ks, st = get_kernels_strides(patch_size, spacing)\n",
    "    print(ks, st)\n",
    "    uks = st[1:]\n",
    "    #dynunet hyperparameter\n",
    "   \n",
    "    # ks = [[3, 3, 1], [3, 3, 3], [3, 3, 3], [3, 3, 3]]\n",
    "    # st = [[1, 1, 1], [2, 2, 1], [2, 2, 2], [2, 2, 1]]\n",
    "    uks = st[1:]\n",
    "\n",
    "    # model = UNETR(\n",
    "    # img_size=spatial_size_xyz,\n",
    "    # in_channels=5,\n",
    "    # out_channels=OAR_nums_plus_one,\n",
    "    # ).to(device)\n",
    "    # model = SegResNet(in_channels = 5, out_channels = OAR_nums_plus_one, dropout_prob = 0.3, act=\"LEAKYRELU\").to(device)\n",
    "    model = DynUNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=2,\n",
    "        out_channels=OAR_nums_plus_one,\n",
    "        kernel_size=ks,\n",
    "        strides=st,\n",
    "        upsample_kernel_size=uks,\n",
    "        dropout=0.1,\n",
    "        act_name= \"LEAKYRELU\",\n",
    "        deep_supervision=False\n",
    "    ).to(device)\n",
    "    # model = SwinUNETR(\n",
    "    # img_size=(64, 64, 32),\n",
    "    # in_channels=5,\n",
    "    # out_channels=OAR_nums_plus_one,\n",
    "    # feature_size=48,\n",
    "    # use_checkpoint=False,\n",
    "    # ).to(device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #FOR SINGLE CHANNEL PREDICTION\n",
    "    loss_function = DiceCELoss(to_onehot_y=OAR_nums_plus_one, softmax=True)\n",
    "    initial_lr = 1e-2\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, weight_decay=3e-5, momentum=0.99, nesterov=True)\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=1e-5)\n",
    "    # scheduler = CosineAnnealingNoWarmUpRestarts(optimizer, 5, 2, 1e-2, 0, 0, 0.8)\n",
    "    T_0 = 20\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=2, eta_min=0)\n",
    "    scheduler = PolyLRScheduler(optimizer, initial_lr=initial_lr, max_steps=1000)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "    import gc\n",
    "    import random\n",
    "    def validation(epoch_iterator_val):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_min = -1024\n",
    "            total_max = 3071\n",
    "            bone_min = -1000\n",
    "            bone_max = 2000\n",
    "            soft_min = -160\n",
    "            soft_max = 350\n",
    "            brain_min = -5\n",
    "            brain_max = 65\n",
    "            stroke_min = 15\n",
    "            stroke_max = 45\n",
    "\n",
    "            for batch in epoch_iterator_val:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                val_inputs, val_labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "                \n",
    "                tx =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "                # x1 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "                x2 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "                # x3 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "                # x4 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "                tx[tx<total_min] = total_min\n",
    "                tx[tx>total_max] = total_max\n",
    "                tx = (tx-total_min)/(total_max-total_min)\n",
    "                # x1[x1<bone_min] = bone_min\n",
    "                # x1[x1>bone_max] = bone_max\n",
    "                # x1 = (x1-bone_min)/(bone_max-bone_min)\n",
    "                x2[x2<soft_min] = soft_min\n",
    "                x2[x2>soft_max] = soft_max\n",
    "                x2 = (x2-soft_min)/(soft_max-soft_min)\n",
    "                # x3[x3<brain_min] = brain_min\n",
    "                # x3[x3>brain_max] = brain_max \n",
    "                # x3 = (x3-brain_min)/(brain_max-brain_min)\n",
    "                # x4[x4<stroke_min] = stroke_min\n",
    "                # x4[x4>stroke_max] = stroke_max\n",
    "                # x4 = (x4 - stroke_min)/(stroke_max - stroke_min)\n",
    "                \n",
    "                # tx *= 10\n",
    "                # x2 *= 2\n",
    "\n",
    "\n",
    "                val_inputs = torch.cat((tx, x2), 1)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    val_outputs = sliding_window_inference(val_inputs, spatial_size_xyz, 4, model)\n",
    "                val_labels_list = decollate_batch(val_labels)\n",
    "                val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "                val_outputs_list = decollate_batch(val_outputs)\n",
    "                val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "                dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "                epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))\n",
    "            mean_dice_val = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "        return mean_dice_val\n",
    "\n",
    "    from PIL import Image\n",
    "    def train(global_step, train_loader, dice_val_best, global_step_best, t_0):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        # rand_fliper_x = RandFlip(prob=0.5, spatial_axis=0)\n",
    "        # rand_fliper_y = RandFlip(prob=0.1, spatial_axis=1)\n",
    "        # rand_fliper_z = RandFlip(prob=0.1, spatial_axis=2)\n",
    "        randcontrast = RandAdjustContrast(prob=0.3, gamma=(0.7, 1.5))\n",
    "        randintensity = RandShiftIntensity(prob=0.15, offsets=0.4)\n",
    "        #randlowsim = RandSimulateLowResolution(prob=0.25)\n",
    "        epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "        print(optimizer.param_groups[0]['lr'])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            step += 1\n",
    "            x, y = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "            # img = x[0, 0, ...].clone.detach().cpu().numpy()\n",
    "            \n",
    "            #print(x.shape)\n",
    "            #x = torch.transpose(x, 1, 0)\n",
    "           # print(x.shape)\n",
    "           # x = rand_fliper_x(x)\n",
    "          #  x = rand_fliper_y(x)\n",
    "           # x = rand_fliper_z(x)\n",
    "           # x = torch.transpose(x, 1, 0)\n",
    "          #  print(x.shape)\n",
    "            total_min = -1024\n",
    "            total_max = 3071\n",
    "            # bone_min = random.randrange(-1024, -699, 1)\n",
    "            # bone_max = random.randrange(1850, 2151, 1)\n",
    "            soft_min = random.randrange(-250, -149, 1)\n",
    "            soft_max = random.randrange(290, 391, 1)\n",
    "            # brain_min = random.randrange(-10, 0, 1)\n",
    "            # brain_max = random.randrange(40, 91, 1)\n",
    "            # stroke_min = random.randrange(10, 21, 1)\n",
    "            # stroke_max = random.randrange(40, 51, 1)\n",
    "            \n",
    "            tx =  torch.reshape(x[:, 0, :, : ,:].clone().detach(), (x.shape[0], 1, x.shape[2], x.shape[3], x.shape[4]))\n",
    "            # x1 =  torch.reshape(x[:, 0, :, : ,:].clone().detach(), (x.shape[0], 1, x.shape[2], x.shape[3], x.shape[4]))\n",
    "            x2 =  torch.reshape(x[:, 0, :, : ,:].clone().detach(), (x.shape[0], 1, x.shape[2], x.shape[3], x.shape[4]))\n",
    "            # x3 =  torch.reshape(x[:, 0, :, : ,:].clone().detach(), (x.shape[0], 1, x.shape[2], x.shape[3], x.shape[4]))\n",
    "            # x4 =  torch.reshape(x[:, 0, :, : ,:].clone().detach(), (x.shape[0], 1, x.shape[2], x.shape[3], x.shape[4]))\n",
    "            \n",
    "            tx[tx<total_min] = total_min\n",
    "            tx[tx>total_max] = total_max\n",
    "            tx = (tx-total_min)/(total_max-total_min)\n",
    "            \n",
    "            # x1[x1<bone_min] = bone_min\n",
    "            # x1[x1>bone_max] = bone_max\n",
    "            # x1 = (x1-bone_min)/(bone_max-bone_min)\n",
    "            x2[x2<soft_min] = soft_min\n",
    "            x2[x2>soft_max] = soft_max\n",
    "            x2 = (x2-soft_min)/(soft_max-soft_min)\n",
    "            tx = randintensity(tx)\n",
    "            tx = randcontrast(tx)\n",
    "            \n",
    "            x2 = randintensity(x2)\n",
    "            x2 = randcontrast(x2)\n",
    "            \n",
    "            \n",
    "            # x3[x3<brain_min] = brain_min\n",
    "            # x3[x3>brain_max] = brain_max \n",
    "            # x3 = (x3-brain_min)/(brain_max-brain_min)\n",
    "            # x4[x4 < stroke_min] = stroke_min\n",
    "            # x4[x4 > stroke_max] = stroke_max\n",
    "            # x4 = (x4 - stroke_min)/(stroke_max - stroke_min)\n",
    "            # tx *= 10\n",
    "            # x2 *= 2\n",
    "\n",
    "            x = torch.cat((tx, x2), 1)\n",
    "            #x = randlowsim(x)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logit_map = model(x)\n",
    "                loss = loss_function(logit_map, y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            epoch_loss += loss.item()\n",
    "            scaler.unscale_(optimizer)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_iterator.set_description(f\"Training ({global_step} / {max_iterations} Steps) (loss={loss:2.5f})\")\n",
    "            if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
    "                epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
    "                dice_val = validation(epoch_iterator_val)\n",
    "                epoch_loss /= step\n",
    "                epoch_loss_values.append(epoch_loss)\n",
    "                metric_values.append(dice_val)\n",
    "                if dice_val > dice_val_best:\n",
    "                    dice_val_best = dice_val\n",
    "                    global_step_best = global_step\n",
    "                    torch.save({\n",
    "                        'global_step': global_step,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    }, os.path.join(model_savepath, \"zone1_model_epoch_SGD_%d_%.3f.pth\" %(global_step, float(dice_val))))\n",
    "                    print(\n",
    "                        \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(dice_val_best, dice_val)\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                            dice_val_best, dice_val\n",
    "                        )\n",
    "                    )\n",
    "                print(optimizer.param_groups[0]['lr'])\n",
    "                # scheduler.step()\n",
    "            # if global_step % 250 == 0 and global_step != 0:\n",
    "                scheduler.step()\n",
    "                print(\"scheduler is steped!\")\n",
    "            global_step += 1\n",
    "        # print((global_step+1) // (len(train_ds)) % t_0)\n",
    "        # if (global_step+1) // (len(train_ds)) % t_0 == 0:\n",
    "        #     print(\"donedone\")\n",
    "        #     scheduler.base_lrs[0] = scheduler.base_lrs[0] * (0.7)\n",
    "        #     t_0 *= 2\n",
    "        \n",
    "        return global_step, dice_val_best, global_step_best, t_0\n",
    "     \n",
    "    max_iterations = 250000\n",
    "    eval_num = 250\n",
    "    post_label = AsDiscrete(to_onehot=OAR_nums_plus_one)\n",
    "    post_pred = AsDiscrete(argmax=True, to_onehot=OAR_nums_plus_one)\n",
    "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "    global_step = 0\n",
    "    dice_val_best = 0.0\n",
    "    global_step_best = 0\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "    t_0 = T_0\n",
    "    while global_step < max_iterations:\n",
    "        global_step, dice_val_best, global_step_best, t_0 = train(global_step, train_loader, dice_val_best, global_step_best, t_0=t_0)\n",
    "        epoch_loss_npy = np.array(epoch_loss_values)\n",
    "        np.save(os.path.join(loss_savepath, \"%d_loss.npy\" %(int(global_step))), epoch_loss_npy)\n",
    "    # total_case_num = 24\n",
    "    # model.load_state_dict(torch.load(os.path.join(root_dir, name_oar.lower() + \"_model_fold0_0.pth\")))\n",
    "    # model.eval()\n",
    "    # original_nib_path = root_dir + r\"/imcroppedval\"\n",
    "    # original_nib_path_list = os.listdir(original_nib_path)\n",
    "    return None\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for case_num in range(total_case_num):\n",
    "    #         start = time.time()\n",
    "    #         template_nib = nib.load(os.path.join(original_nib_path, original_nib_path_list[case_num]))\n",
    "\n",
    "    #         img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
    "    #         img = val_ds[case_num][\"image\"]\n",
    "    #         label = val_ds[case_num][\"label\"]\n",
    "    #         val_inputs = torch.unsqueeze(img, 0).cuda()\n",
    "    #         val_labels = torch.unsqueeze(label, 0).cuda()\n",
    "    #         bone_min = -1000\n",
    "    #         bone_max = 2000\n",
    "    #         soft_min = -160\n",
    "    #         soft_max = 350\n",
    "    #         brain_min = -5\n",
    "    #         brain_max = 65\n",
    "    #         stroke_min = 15\n",
    "    #         stroke_max = 45\n",
    "\n",
    "    #         # box_start, box_end = FG_cropper.compute_bounding_box(val_inputs)\n",
    "    #         tx =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x1 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x2 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x3 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x4 =  torch.reshape(val_inputs[:, 0, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         tx2 = torch.reshape(val_inputs[:, 1, :, :, :].clone().detach(), (val_inputs.shape[0], 1, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]))\n",
    "    #         x1[x1<bone_min] = bone_min\n",
    "    #         x1[x1>bone_max] = bone_max\n",
    "    #         x1 = (x1-bone_min)/(bone_max-bone_min)\n",
    "    #         x2[x2<soft_min] = soft_min\n",
    "    #         x2[x2>soft_max] = soft_max\n",
    "    #         x2 = (x2-soft_min)/(soft_max-soft_min)\n",
    "    #         x3[x3<brain_min] = brain_min\n",
    "    #         x3[x3>brain_max] = brain_max \n",
    "    #         x3 = (x3-brain_min)/(brain_max-brain_min)\n",
    "    #         x4[x4<stroke_min] = stroke_min\n",
    "    #         x4[x4>stroke_max] = stroke_max\n",
    "    #         x4 = (x4 - stroke_min)/(stroke_max - stroke_min)\n",
    "    #         val_inputs = torch.cat((tx, x1, x2, x3, x4, tx2), 1)\n",
    "\n",
    "    #         val_outputs = sliding_window_inference(val_inputs, spatial_size_xyz, 4, model, overlap=0.5)\n",
    "    #         last_outputs = torch.argmax(val_outputs, dim=1).detach().cpu()[0].numpy()\n",
    "    #         img_npy = img.cpu()[0].numpy()\n",
    "    #         label_npy = label.cpu()[0].numpy()\n",
    "    #         print(\"time taken : %f\" %(time.time()- start))\n",
    "    #         # nib.save(\n",
    "    #                 # nib.Nifti1Image(img_npy.astype(np.uint8), np.ones((4, 4))), os.path.join(r\"D:\\!HaN_Challenge\\HaN-Seg_NRRD\\!output_dir\\label_1to10_fold0\", \"val_ct_%02d\" %(case_num+1))\n",
    "    #             # )\n",
    "\n",
    "    #         nib.save(\n",
    "    #                 nib.Nifti1Image(label_npy.astype(np.uint8), template_nib.affine, template_nib.header), os.path.join(root_dir + \"/result\", \"val_label_%02d\" %(case_num+1))\n",
    "    #             )\n",
    "    #         nib.save(\n",
    "    #                 nib.Nifti1Image(last_outputs.astype(np.uint8), template_nib.affine, template_nib.header), os.path.join(root_dir + \"/result\", \"infered_label_%02d\" %(case_num+1))\n",
    "    #             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (248240 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.44s/it]\n",
      "Training (248250 / 250003 Steps) (loss=0.70771):  28%|██▊       | 11/40 [00:17<02:15,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.7193235754966736\n",
      "0.0001296525277354211\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248279 / 250003 Steps) (loss=0.69172): 100%|██████████| 40/40 [00:24<00:00,  1.61it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011497098073005987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248319 / 250003 Steps) (loss=0.74168): 100%|██████████| 40/40 [00:10<00:00,  3.68it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011497098073005987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248359 / 250003 Steps) (loss=0.71316): 100%|██████████| 40/40 [00:10<00:00,  3.78it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011497098073005987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248399 / 250003 Steps) (loss=0.75651): 100%|██████████| 40/40 [00:10<00:00,  3.85it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011497098073005987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248439 / 250003 Steps) (loss=0.78851): 100%|██████████| 40/40 [00:10<00:00,  3.81it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011497098073005987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248479 / 250003 Steps) (loss=0.72121): 100%|██████████| 40/40 [00:10<00:00,  3.72it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011497098073005987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (248480 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.43s/it]5<00:06,  3.19it/s]\n",
      "Training (248500 / 250003 Steps) (loss=0.86555):  52%|█████▎    | 21/40 [00:19<01:26,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.718608558177948\n",
      "0.00011497098073005987\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248519 / 250003 Steps) (loss=0.46363): 100%|██████████| 40/40 [00:24<00:00,  1.65it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010007742567915822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248559 / 250003 Steps) (loss=0.78031): 100%|██████████| 40/40 [00:10<00:00,  3.80it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010007742567915822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248599 / 250003 Steps) (loss=0.85167): 100%|██████████| 40/40 [00:10<00:00,  3.78it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010007742567915822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248639 / 250003 Steps) (loss=0.76454): 100%|██████████| 40/40 [00:10<00:00,  3.89it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010007742567915822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248679 / 250003 Steps) (loss=0.71817): 100%|██████████| 40/40 [00:10<00:00,  3.80it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010007742567915822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248719 / 250003 Steps) (loss=0.85309): 100%|██████████| 40/40 [00:10<00:00,  3.72it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010007742567915822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (248720 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.45s/it]8<00:02,  3.89it/s]\n",
      "Training (248750 / 250003 Steps) (loss=0.66954):  78%|███████▊  | 31/40 [00:23<00:41,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.7209955453872681\n",
      "0.00010007742567915822\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248759 / 250003 Steps) (loss=0.47115): 100%|██████████| 40/40 [00:24<00:00,  1.60it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.493232323171241e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248799 / 250003 Steps) (loss=0.79966): 100%|██████████| 40/40 [00:10<00:00,  3.68it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.493232323171241e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248839 / 250003 Steps) (loss=0.70922): 100%|██████████| 40/40 [00:10<00:00,  3.71it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.493232323171241e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248879 / 250003 Steps) (loss=0.67338): 100%|██████████| 40/40 [00:10<00:00,  3.67it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.493232323171241e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248919 / 250003 Steps) (loss=0.62173): 100%|██████████| 40/40 [00:10<00:00,  3.79it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.493232323171241e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248959 / 250003 Steps) (loss=0.76963): 100%|██████████| 40/40 [00:10<00:00,  3.86it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.493232323171241e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (248999 / 250003 Steps) (loss=0.83758): 100%|██████████| 40/40 [00:10<00:00,  3.85it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.493232323171241e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (249000 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.44s/it]<?, ?it/s]\n",
      "Training (249000 / 250003 Steps) (loss=0.77967):   2%|▎         | 1/40 [00:14<09:37, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.7209019660949707\n",
      "8.493232323171241e-05\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249039 / 250003 Steps) (loss=0.69797): 100%|██████████| 40/40 [00:24<00:00,  1.61it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.947906928878752e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249079 / 250003 Steps) (loss=0.76138): 100%|██████████| 40/40 [00:10<00:00,  3.80it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.947906928878752e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249119 / 250003 Steps) (loss=0.85281): 100%|██████████| 40/40 [00:10<00:00,  3.65it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.947906928878752e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249159 / 250003 Steps) (loss=0.70730): 100%|██████████| 40/40 [00:10<00:00,  3.68it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.947906928878752e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249199 / 250003 Steps) (loss=0.72435): 100%|██████████| 40/40 [00:10<00:00,  3.86it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.947906928878752e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249239 / 250003 Steps) (loss=0.76785): 100%|██████████| 40/40 [00:11<00:00,  3.60it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.947906928878752e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (249240 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.43s/it]2<00:07,  3.88it/s]\n",
      "Training (249250 / 250003 Steps) (loss=0.76535):  28%|██▊       | 11/40 [00:17<02:14,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.7190843224525452\n",
      "6.947906928878752e-05\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249279 / 250003 Steps) (loss=0.65471): 100%|██████████| 40/40 [00:24<00:00,  1.61it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3630164520934975e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249319 / 250003 Steps) (loss=0.66337): 100%|██████████| 40/40 [00:10<00:00,  3.85it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3630164520934975e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249359 / 250003 Steps) (loss=0.86145): 100%|██████████| 40/40 [00:10<00:00,  3.90it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3630164520934975e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249399 / 250003 Steps) (loss=0.77716): 100%|██████████| 40/40 [00:10<00:00,  3.78it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3630164520934975e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249439 / 250003 Steps) (loss=0.71690): 100%|██████████| 40/40 [00:10<00:00,  3.91it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3630164520934975e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249479 / 250003 Steps) (loss=0.68623): 100%|██████████| 40/40 [00:10<00:00,  3.84it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3630164520934975e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (249480 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.45s/it]5<00:05,  3.68it/s]\n",
      "Training (249500 / 250003 Steps) (loss=0.56371):  52%|█████▎    | 21/40 [00:20<01:27,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.7190069556236267\n",
      "5.3630164520934975e-05\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249519 / 250003 Steps) (loss=0.70437): 100%|██████████| 40/40 [00:24<00:00,  1.60it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.723291133272141e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249559 / 250003 Steps) (loss=0.65394): 100%|██████████| 40/40 [00:11<00:00,  3.58it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.723291133272141e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249599 / 250003 Steps) (loss=0.72901): 100%|██████████| 40/40 [00:10<00:00,  3.67it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.723291133272141e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249639 / 250003 Steps) (loss=0.75574): 100%|██████████| 40/40 [00:10<00:00,  3.81it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.723291133272141e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249679 / 250003 Steps) (loss=0.80680): 100%|██████████| 40/40 [00:10<00:00,  3.74it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.723291133272141e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249719 / 250003 Steps) (loss=0.88142): 100%|██████████| 40/40 [00:11<00:00,  3.62it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.723291133272141e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (249720 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.43s/it]8<00:02,  3.36it/s]\n",
      "Training (249750 / 250003 Steps) (loss=0.78141):  78%|███████▊  | 31/40 [00:22<00:41,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.7189099788665771\n",
      "3.723291133272141e-05\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249759 / 250003 Steps) (loss=0.10093): 100%|██████████| 40/40 [00:24<00:00,  1.61it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.995262314968881e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249799 / 250003 Steps) (loss=0.76087): 100%|██████████| 40/40 [00:11<00:00,  3.61it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.995262314968881e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249839 / 250003 Steps) (loss=0.69880): 100%|██████████| 40/40 [00:10<00:00,  3.85it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.995262314968881e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249879 / 250003 Steps) (loss=0.51511): 100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.995262314968881e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249919 / 250003 Steps) (loss=0.84904): 100%|██████████| 40/40 [00:10<00:00,  3.79it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.995262314968881e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249959 / 250003 Steps) (loss=0.80142): 100%|██████████| 40/40 [00:10<00:00,  3.90it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.995262314968881e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (249999 / 250003 Steps) (loss=0.73807): 100%|██████████| 40/40 [00:10<00:00,  3.74it/s]\n",
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.995262314968881e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (250000 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.44s/it]<?, ?it/s]\n",
      "Training (250000 / 250003 Steps) (loss=0.69158):   2%|▎         | 1/40 [00:14<09:41, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.7186266183853149\n",
      "1.995262314968881e-05\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate (250000 / 10 Steps): 100%|██████████| 10/10 [00:14<00:00,  1.44s/it]<02:09,  3.50s/it]\n",
      "Training (250003 / 250003 Steps) (loss=0.46903):  10%|█         | 4/40 [00:29<04:43,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Was Not Saved ! Current Best Avg. Dice: 0.7257477641105652 Current Avg. Dice: 0.7186266183853149\n",
      "0.0\n",
      "scheduler is steped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (250003 / 250003 Steps) (loss=0.46903):  10%|█         | 4/40 [00:30<04:31,  7.53s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For non-complex input tensors, argument alpha must not be a complex number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m json_name_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_f0_fold00.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_f0_fold01.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_f0_fold02.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      5\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_f0_fold03.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_f0_fold04.json\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m json \u001b[38;5;129;01min\u001b[39;00m json_name_list:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43moar_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 360\u001b[0m, in \u001b[0;36moar_run_train\u001b[0;34m(json_name)\u001b[0m\n\u001b[1;32m    358\u001b[0m t_0 \u001b[38;5;241m=\u001b[39m T_0\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m global_step \u001b[38;5;241m<\u001b[39m max_iterations:\n\u001b[0;32m--> 360\u001b[0m     global_step, dice_val_best, global_step_best, t_0 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdice_val_best\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step_best\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     epoch_loss_npy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(epoch_loss_values)\n\u001b[1;32m    362\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(loss_savepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_loss.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m(\u001b[38;5;28mint\u001b[39m(global_step))), epoch_loss_npy)\n",
      "Cell \u001b[0;32mIn[3], line 306\u001b[0m, in \u001b[0;36moar_run_train.<locals>.train\u001b[0;34m(global_step, train_loader, dice_val_best, global_step_best, t_0)\u001b[0m\n\u001b[1;32m    304\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    305\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m--> 306\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    308\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunetpet/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunetpet/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:350\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 350\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunetpet/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunetpet/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunetpet/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunetpet/lib/python3.10/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunetpet/lib/python3.10/site-packages/torch/optim/sgd.py:220\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 220\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunetpet/lib/python3.10/site-packages/torch/optim/sgd.py:327\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    324\u001b[0m         device_grads \u001b[38;5;241m=\u001b[39m bufs\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_has_sparse_grad:\n\u001b[0;32m--> 327\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;66;03m# foreach APIs don't support sparse\u001b[39;00m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(device_params)):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For non-complex input tensors, argument alpha must not be a complex number."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "json_name_list = [\n",
    "                #\"dataset_f0_fold00.json\", \n",
    "                \"dataset_f0_fold01.json\", \n",
    "                \"dataset_f0_fold02.json\", \n",
    "                \"dataset_f0_fold03.json\", \n",
    "                \"dataset_f0_fold04.json\"]\n",
    "for json in json_name_list:\n",
    "    oar_run_train(json)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fold 0 \n",
    "arytenoid 0.64386\n",
    "a_carotid_l 0.83728 (아마이것보단 높을것)\n",
    "a_carotid_r 0.88483\n",
    "bone_mandible 0.964496\n",
    "oralcavity 0.924\n",
    "cochlea_l 0.85533\n",
    "optnr_1 0.77\n",
    "\n",
    "fold 1 \n",
    "arytenoid 0.45... 가우시안 노이즈 + 가우시안 블러링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swinjupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
